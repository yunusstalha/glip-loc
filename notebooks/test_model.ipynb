{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# If needed, add the project root to sys.path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from models.glip_loc import GLIPLocModel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prepare dummy data\n",
    "B = 4\n",
    "ground_images = torch.randn(B, 3, 224, 224).to(device)\n",
    "sat_images = torch.randn(B, 3, 224, 224).to(device)\n",
    "# For testing separate captions:\n",
    "ground_captions = [\n",
    "    \"A ground-level view of a busy street\",\n",
    "    \"A ground photo of a forest trail\",\n",
    "    \"A panorama of a rural village at ground level\",\n",
    "    \"A ground-level shot of a modern building\"\n",
    "]\n",
    "sat_captions = [\n",
    "    \"A satellite view of a city center\",\n",
    "    \"A satellite image of a large forest\",\n",
    "    \"A top-down satellite shot of farmland\",\n",
    "    \"A satellite image of coastal lines\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: CLIP Vision Only ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Embeddings (CLIP Vision Only): torch.Size([4, 512])\n",
      "Satellite Embeddings (CLIP Vision Only): torch.Size([4, 512])\n",
      "Ground Embeddings (CLIP Vision Only): torch.Size([4, 512])\n",
      "Satellite Embeddings (CLIP Vision Only): torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# 1. CLIP Vision Only\n",
    "###################################\n",
    "print(\"=== Test 1: CLIP Vision Only ===\")\n",
    "clip_vision_model = GLIPLocModel(\n",
    "    model_name=\"openai/clip-vit-base-patch32\",\n",
    "    pretrained=True,\n",
    "    use_text=False\n",
    ").to(device)\n",
    "clip_vision_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get ground embeddings only\n",
    "    ground_emb = clip_vision_model(ground_image=ground_images)\n",
    "    # Get satellite embeddings only\n",
    "    sat_emb = clip_vision_model(satellite_image=sat_images)\n",
    "\n",
    "    # Get both ground and satellite embeddings\n",
    "    ground_emb_, sat_emb_ = clip_vision_model(ground_image=ground_images, satellite_image=sat_images)\n",
    "\n",
    "print(\"Ground Embeddings (CLIP Vision Only):\", ground_emb.shape)  # Expect [4, 768]\n",
    "print(\"Satellite Embeddings (CLIP Vision Only):\", sat_emb.shape)  # Expect [4, 768]\n",
    "print(\"Ground Embeddings (CLIP Vision Only):\", ground_emb_.shape)  # Expect [4, 768]\n",
    "print(\"Satellite Embeddings (CLIP Vision Only):\", sat_emb_.shape)  # Expect [4, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: CLIP Vision + Text ===\n",
      "Ground Emb (CLIP V+T): torch.Size([4, 512])\n",
      "Sat Emb (CLIP V+T): torch.Size([4, 512])\n",
      "Ground Text Emb (CLIP V+T): torch.Size([4, 512])\n",
      "Sat Text Emb (CLIP V+T): torch.Size([4, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# 2. CLIP Vision + Text\n",
    "###################################\n",
    "print(\"=== Test 2: CLIP Vision + Text ===\")\n",
    "clip_vision_text_model = GLIPLocModel(\n",
    "    model_name=\"openai/clip-vit-base-patch32\",\n",
    "    pretrained=True,\n",
    "    use_text=True\n",
    ").to(device)\n",
    "clip_vision_text_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ground_emb_clip, sat_emb_clip, ground_txt_emb_clip, sat_txt_emb_clip = clip_vision_text_model(\n",
    "        ground_image=ground_images, \n",
    "        satellite_image=sat_images,\n",
    "        ground_captions=ground_captions,\n",
    "        satellite_captions=sat_captions\n",
    "    )\n",
    "\n",
    "print(\"Ground Emb (CLIP V+T):\", ground_emb_clip.shape)\n",
    "print(\"Sat Emb (CLIP V+T):\", sat_emb_clip.shape)\n",
    "print(\"Ground Text Emb (CLIP V+T):\", ground_txt_emb_clip.shape)\n",
    "print(\"Sat Text Emb (CLIP V+T):\", sat_txt_emb_clip.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 3: ConvNeXt Vision Only ===\n",
      "Ground Emb (ConvNeXt Vision): torch.Size([4, 1024])\n",
      "Sat Emb (ConvNeXt Vision): torch.Size([4, 1024])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# 3. ConvNeXt Vision Only\n",
    "###################################\n",
    "print(\"=== Test 3: ConvNeXt Vision Only ===\")\n",
    "convnext_vision_model = GLIPLocModel(\n",
    "    model_name=\"convnext_base\",\n",
    "    pretrained=True,\n",
    "    use_text=False\n",
    ").to(device)\n",
    "convnext_vision_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ground_emb_conv, sat_emb_conv = convnext_vision_model(\n",
    "        ground_image=ground_images, \n",
    "        satellite_image=sat_images\n",
    "    )\n",
    "\n",
    "print(\"Ground Emb (ConvNeXt Vision):\", ground_emb_conv.shape)\n",
    "print(\"Sat Emb (ConvNeXt Vision):\", sat_emb_conv.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 4: ConvNeXt Vision + Text ===\n",
      "Ground Emb (CLIP V+T): torch.Size([4, 512])\n",
      "Sat Emb (CLIP V+T): torch.Size([4, 512])\n",
      "Ground Text Emb (CLIP V+T): torch.Size([4, 512])\n",
      "Sat Text Emb (CLIP V+T): torch.Size([4, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# 4. ConvNeXt Vision + Text\n",
    "###################################\n",
    "print(\"=== Test 4: ConvNeXt Vision + Text ===\")\n",
    "convnext_vision_text_model = GLIPLocModel(\n",
    "    model_name=\"convnext_base\",\n",
    "    pretrained=True,\n",
    "    use_text=True\n",
    ").to(device)\n",
    "convnext_vision_text_model.eval()\n",
    "with torch.no_grad():\n",
    "    ground_emb_clip, sat_emb_clip, ground_txt_emb_clip, sat_txt_emb_clip = convnext_vision_text_model(\n",
    "        ground_image=ground_images, \n",
    "        satellite_image=sat_images,\n",
    "        ground_captions=ground_captions,\n",
    "        satellite_captions=sat_captions\n",
    "    )\n",
    "\n",
    "print(\"Ground Emb (CLIP V+T):\", ground_emb_clip.shape)\n",
    "print(\"Sat Emb (CLIP V+T):\", sat_emb_clip.shape)\n",
    "print(\"Ground Text Emb (CLIP V+T):\", ground_txt_emb_clip.shape)\n",
    "print(\"Sat Text Emb (CLIP V+T):\", sat_txt_emb_clip.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP-LoRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
